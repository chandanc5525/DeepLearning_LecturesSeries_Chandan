{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7773b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print('Model Architecture Design for Machine Learning')\n",
    "\n",
    "# ======================\n",
    "# DATA LAYER (Input)\n",
    "# ======================\n",
    "def data_ingestion(file_path=None):\n",
    "    \"\"\"\n",
    "    ---------------------------------------------------------------------\n",
    "    Function: data_ingestion\n",
    "    ---------------------------------------------------------------------\n",
    "    Purpose:\n",
    "        - Load dataset from a CSV file or create a dummy dataset.\n",
    "        - Serves as the first layer of the ML pipeline (data input layer).\n",
    "        - Ensures flexibility: can handle tabular datasets or simulated data.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path: str or None\n",
    "            Path to CSV file. If None, generates random dummy dataset.\n",
    "    \n",
    "    Returns:\n",
    "        df: pandas.DataFrame\n",
    "            Loaded or generated dataset.\n",
    "    ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    if file_path:\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        # Create a dummy dataset with 1000 samples and 20 features\n",
    "        X_dummy = np.random.rand(1000, 20)\n",
    "        y_dummy = np.random.randint(0, 2, size=(1000, 1))  # Binary classification\n",
    "        df = pd.DataFrame(np.hstack((X_dummy, y_dummy)),\n",
    "                          columns=[f'feat_{i}' for i in range(20)] + ['target'])\n",
    "    return df\n",
    "\n",
    "def get_X_y(df, target_col='target'):\n",
    "    \"\"\"\n",
    "    ---------------------------------------------------------------------\n",
    "    Function: get_X_y\n",
    "    ---------------------------------------------------------------------\n",
    "    Purpose:\n",
    "        - Split dataset into features (X) and target (y)\n",
    "        - Keeps pipeline modular for later transformations\n",
    "    \n",
    "    Parameters:\n",
    "        df: pandas.DataFrame\n",
    "        target_col: str, name of target column\n",
    "    \n",
    "    Returns:\n",
    "        X: np.ndarray of features\n",
    "        y: np.ndarray of target\n",
    "    ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=[target_col]).values\n",
    "    y = df[target_col].values\n",
    "    return X, y\n",
    "\n",
    "# ======================\n",
    "# FEATURE LAYER (Engineering)\n",
    "# ======================\n",
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    ---------------------------------------------------------------------\n",
    "    Function: feature_engineering\n",
    "    ---------------------------------------------------------------------\n",
    "    Purpose:\n",
    "        - Apply preprocessing on the features (e.g., scaling, normalization)\n",
    "        - Important for ANN since large feature value differences can destabilize training\n",
    "    \n",
    "    Notes:\n",
    "        - StandardScaler centers data (mean=0) and scales to unit variance\n",
    "        - Optional: add feature selection, polynomial features, or dimensionality reduction\n",
    "    ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    feature_cols = df.columns[:-1]  # Exclude target column\n",
    "    df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
    "    return df\n",
    "\n",
    "def split_data(X, y, test_size=0.2, random_state=42, one_hot=False, num_classes=2):\n",
    "    \"\"\"\n",
    "    ---------------------------------------------------------------------\n",
    "    Function: split_data\n",
    "    ---------------------------------------------------------------------\n",
    "    Purpose:\n",
    "        - Split dataset into training and testing sets\n",
    "        - Optionally one-hot encode targets for multi-class classification\n",
    "    \n",
    "    Parameters:\n",
    "        X: np.ndarray, features\n",
    "        y: np.ndarray, targets\n",
    "        test_size: float, fraction for test split\n",
    "        one_hot: bool, whether to one-hot encode y\n",
    "        num_classes: int, required if one_hot=True\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test: np.ndarrays\n",
    "    ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    if one_hot:\n",
    "        y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "        y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# ======================\n",
    "# MODEL LAYER (ML Core)\n",
    "# ======================\n",
    "def train_model(X_train, y_train, input_dim=None, num_classes=2, epochs=20, batch_size=32):\n",
    "    \"\"\"\n",
    "    ---------------------------------------------------------------------\n",
    "    Function: train_model\n",
    "    ---------------------------------------------------------------------\n",
    "    Purpose:\n",
    "        - Build and train an Artificial Neural Network (ANN)\n",
    "        - Uses Dense layers, Dropout, and configurable output activation\n",
    "    \n",
    "    Architecture Notes:\n",
    "        - Input layer: input_dim = number of features\n",
    "        - Hidden layers: ReLU activation, Dropout for regularization\n",
    "        - Output layer:\n",
    "            - Binary classification: Dense(1, sigmoid)\n",
    "            - Multi-class classification: Dense(num_classes, softmax)\n",
    "    \n",
    "    Loss Functions:\n",
    "        - Binary: binary_crossentropy\n",
    "        - Multi-class: categorical_crossentropy\n",
    "        - Explanation:\n",
    "            * Loss measures difference between predicted probabilities and true labels\n",
    "            * Lower loss â†’ better predictions\n",
    "    \n",
    "    Optimizer:\n",
    "        - Adam: adaptive learning rate, works well in practice\n",
    "    \n",
    "    Returns:\n",
    "        model: trained tf.keras.Model\n",
    "    ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    if input_dim is None:\n",
    "        input_dim = X_train.shape[1]\n",
    "    \n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_dim,)),  # Hidden layer 1\n",
    "        Dropout(0.3),  # Prevent overfitting\n",
    "        Dense(32, activation='relu'),  # Hidden layer 2\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax' if num_classes>2 else 'sigmoid')  # Output\n",
    "    ])\n",
    "    \n",
    "    # Set appropriate loss\n",
    "    loss_fn = 'categorical_crossentropy' if num_classes>2 else 'binary_crossentropy'\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    ---------------------------------------------------------------------\n",
    "    Function: evaluate_model\n",
    "    ---------------------------------------------------------------------\n",
    "    Purpose:\n",
    "        - Evaluate the ANN performance on the test set\n",
    "        - Returns loss and accuracy\n",
    "        - Prints results for quick reference\n",
    "    ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Loss: {score[0]:.4f}, Test Accuracy: {score[1]:.4f}\")\n",
    "    return score\n",
    "\n",
    "def save_model(model, filename='model.h5'):\n",
    "    \"\"\"\n",
    "    ---------------------------------------------------------------------\n",
    "    Function: save_model\n",
    "    ---------------------------------------------------------------------\n",
    "    Purpose:\n",
    "        - Save the trained model to disk for future inference\n",
    "        - .h5 format compatible with TensorFlow/Keras\n",
    "    ---------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    model.save(filename)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "# ======================\n",
    "# ORCHESTRATION LAYER\n",
    "# ======================\n",
    "# Step 1: Load data\n",
    "df = data_ingestion()\n",
    "\n",
    "# Step 2: Feature engineering\n",
    "df = feature_engineering(df)\n",
    "\n",
    "# Step 3: Extract features and target\n",
    "X, y = get_X_y(df)\n",
    "\n",
    "# Step 4: Split data into train/test\n",
    "X_train, X_test, y_train, y_test = split_data(X, y, one_hot=False, num_classes=2)\n",
    "\n",
    "# Step 5: Train the ANN model\n",
    "model = train_model(X_train, y_train, num_classes=2, epochs=10, batch_size=32)\n",
    "\n",
    "# Step 6: Evaluate model\n",
    "score = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "# Step 7: Save model\n",
    "save_model(model, 'ann_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
